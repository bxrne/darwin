@book{Goldberg1989Genetic,
  title     = {Genetic Algorithms in Search, Optimization and Machine Learning},
  author    = {David E. Goldberg},
  year      = {1989},
  publisher = {Addison-Wesley},
  address   = {Reading, MA},
  isbn      = {0-201-15767-5}
}

@article{Koza1992Genetic,
  title     = {Genetic Programming: On the Programming of Computers by Means of Natural Selection},
  author    = {John R. Koza},
  journal   = {MIT Press},
  year      = {1992},
  publisher = {MIT Press},
  address   = {Cambridge, MA}
}

@article{ONeill2001Grammatical,
  title     = {Grammatical Evolution},
  author    = {Michael O'Neill and Conor Ryan},
  journal   = {IEEE Transactions on Evolutionary Computation},
  volume    = {5},
  number    = {4},
  pages     = {349--358},
  year      = {2001},
  publisher = {IEEE}
}

@article{Deb2002Fast,
  title     = {A Fast and Elitist Multiobjective Genetic Algorithm: NSGA-II},
  author    = {Kalyanmoy Deb and Amrit Pratap and Sameer Agarwal and T. Meyarivan},
  journal   = {IEEE Transactions on Evolutionary Computation},
  volume    = {6},
  number    = {2},
  pages     = {182--197},
  year      = {2002},
  publisher = {IEEE}
}

@article{Petroski2018Deep,
  title     = {Deep Neuroevolution: Genetic Algorithms are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning},
  author    = {Felipe Petroski Such and Vashisht Madhavan and Edoardo Conti and Joel Lehman and Kenneth O. Stanley and Jeff Clune},
  journal   = {arXiv preprint arXiv:1712.06567},
  year      = {2018}
}

@inproceedings{Gold2023Genetic,
  title     = {Genetic Programming and Coevolution to Play the Bomberman Video Game},
  author    = {R. Gold and others},
  booktitle = {Applications of Evolutionary Computation},
  pages     = {765--779},
  year      = {2023},
  publisher = {Springer},
  doi       = {10.1007/978-3-031-30229-9_49}
}

@misc{Wiggins2012Twelve,
  title     = {The Twelve-Factor App},
  author    = {Adam Wiggins},
  year      = {2012},
  url       = {https://12factor.net/},
  note      = {Accessed: 2025}
}

@article{stanley_evolving_2002,
	title = {Evolving {Neural} {Networks} through {Augmenting} {Topologies}},
	volume = {10},
	issn = {1063-6560, 1530-9304},
	url = {https://direct.mit.edu/evco/article/10/2/99-127/1123},
	doi = {10.1162/106365602320169811},
	abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best ﬁxed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efﬁciency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is signiﬁcantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
	language = {en},
	number = {2},
	urldate = {2025-12-12},
	journal = {Evolutionary Computation},
	author = {Stanley, Kenneth O. and Miikkulainen, Risto},
	month = jun,
	year = {2002},
	pages = {99--127},
	file = {PDF:/home/art/Zotero/storage/EW7XVKE5/Stanley and Miikkulainen - 2002 - Evolving Neural Networks through Augmenting Topologies.pdf:application/pdf},
}

@inproceedings{keijzer_improving_2003,
	address = {Berlin, Heidelberg},
	title = {Improving {Symbolic} {Regression} with {Interval} {Arithmetic} and {Linear} {Scaling}},
	isbn = {978-3-540-36599-0},
	abstract = {The use of protected operators and squared error measures are standard approaches in symbolic regression. It will be shown that two relatively minor modifications of a symbolic regression system can result in greatly improved predictive performance and reliability of the induced expressions. To achieve this, interval arithmetic and linear scaling are used. An experimental section demonstrates the improvements on 15 symbolic regression problems.},
	booktitle = {Genetic {Programming}},
	publisher = {Springer Berlin Heidelberg},
	author = {Keijzer, Maarten},
	editor = {Ryan, Conor and Soule, Terence and Keijzer, Maarten and Tsang, Edward and Poli, Riccardo and Costa, Ernesto},
	year = {2003},
	pages = {70--82},
}

@misc{straka_strakamgenerals-bots_2025,
	title = {strakam/generals-bots},
	url = {https://github.com/strakam/generals-bots},
	abstract = {Develop your agent for generals.io!},
	urldate = {2025-12-12},
	author = {Straka, Matej},
	month = dec,
	year = {2025},
	note = {original-date: 2024-06-05T10:44:06Z},
	keywords = {botting, gym, gym-environment, gymnasium, machine-learning, marl, pettingzoo, pygame, python, reinforcement-learning, strategy},
}
